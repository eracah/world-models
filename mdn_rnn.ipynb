{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. set up MDN_RNN with VAE data \n",
    "7. add temperature to MDN-RNN\n",
    "8. add model saving to MDN-RNN\n",
    "9. add sampling from mu and sigma of z for teacher forcing insteado fusing z exactly\n",
    "10. try instead brand new rollouts that are passed thru autoencoder (no need to store anything)\n",
    "10. put in exact alex graves hyperparams\n",
    "11. add weight init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init\n",
    "import time\n",
    "from torch import optim\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "    sys.argv = [\"\"]\n",
    "\n",
    "import gym\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--weights_file\",type=str,default=\"None\")\n",
    "parser.add_argument(\"--lr\",type=float, default=0.0001)\n",
    "parser.add_argument(\"--ctl_type\",type=str, default=\"lstm\")\n",
    "parser.add_argument(\"--opt\",type=str, default=\"adam\")\n",
    "parser.add_argument(\"--iters\",type=int, default=100000)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim(name,  model, lr, momentum=0):\n",
    "    if name == \"adam\":\n",
    "        return optim.Adam(params=model.parameters(),\n",
    "                        lr=lr)\n",
    "    elif name == \"sgd\":\n",
    "        return optim.SGD(params=model.parameters(),\n",
    "                        lr=lr,\n",
    "                        momentum=momentum)\n",
    "    elif name == \"rmsprop\":\n",
    "          return optim.RMSprop(params=model.parameters(),\n",
    "                        lr=lr,\n",
    "                        momentum=momentum)\n",
    "\n",
    "def print_info(mode,loss,t0,it):\n",
    "    print(\"time: %8.4f\"% (time.time() - t0))\n",
    "    print(\"%s Loss for it %i: %8.4f\"%(mode.capitalize(),it,loss))\n",
    "    #print(\"%s Accuracy for epoch %i: %8.4f\"%(mode.capitalize(),epoch,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, input_size,hidden_size,num_layers):\n",
    "       \n",
    "        super(LSTM,self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=input_size,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers=num_layers)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.h_prev = None\n",
    "        self.c_prev = None\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.h_prev = Variable(torch.Tensor(1,self.batch_size,self.hidden_size).normal_()).cuda()\n",
    "        self.c_prev = Variable(torch.Tensor(1,self.batch_size,self.hidden_size).normal_()).cuda()\n",
    "\n",
    "    def forward(self, az):\n",
    "        \n",
    "\n",
    "        lstm_out, (self.h_prev,self.c_prev) = self.rnn(az[None,:],(self.h_prev,self.c_prev))\n",
    "        \n",
    "\n",
    "        return lstm_out, self.h_prev\n",
    "\n",
    "\n",
    "class MDN(nn.Module):\n",
    "    def __init__(self,input_size,output_size,num_gaussians,nz):\n",
    "        super(MDN,self).__init__()\n",
    "\n",
    "        self.nz = nz\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.mdn_fc = nn.Linear(in_features=input_size,\n",
    "                                out_features=output_size)\n",
    "\n",
    "            \n",
    "    def postproc_mdn_out(self,mdn_out):\n",
    "        mu = mdn_out[:,:self.num_gaussians*self.nz]\n",
    "\n",
    "        sigma = mdn_out[:,self.num_gaussians*self.nz:2*self.num_gaussians*self.nz]\n",
    "        \n",
    "        pi = mdn_out[:,-self.num_gaussians:]\n",
    "        \n",
    "        \n",
    "        mu = mu.resize(mu.size(0),self.num_gaussians,self.nz)\n",
    "        \n",
    "        sigma = torch.exp(sigma)\n",
    "        sigma = sigma.resize(sigma.size(0),self.num_gaussians,self.nz)\n",
    "        \n",
    "        pi = F.softmax(pi,dim=1)\n",
    "        return mu, sigma, pi\n",
    "    \n",
    "    def forward(self,lstm_out):\n",
    "        raw_mdn_out = self.mdn_fc(lstm_out)\n",
    "        mu, sigma, pi = self.postproc_mdn_out(raw_mdn_out)\n",
    "        return mu, sigma, pi\n",
    "    \n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self,env=\"CarRacing\", num_gaussians=5,batch_size=128,num_layers=1): \n",
    "        super(M,self).__init__()\n",
    "        if env == \"CarRacing\":\n",
    "            self.nz = 32\n",
    "            self.nh = 256\n",
    "            self.action_len = 3 #3 continuous values\n",
    "        elif env == \"Doom\":\n",
    "            pass # self.nz, self.nh = 64, 512\n",
    "\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.mu_len, self.sigma_len, self.pi_len = self.nz, self.nz, 1\n",
    "\n",
    "        self.len_mdn_output = self.num_gaussians*(self.sigma_len + self.mu_len + self.pi_len)\n",
    "        \n",
    "        self.lstm = LSTM(batch_size=batch_size, \n",
    "                          input_size=self.nz + self.action_len,\n",
    "                          hidden_size=self.nh,\n",
    "                          num_layers=num_layers)\n",
    "        \n",
    "        self.mdn = MDN(input_size=self.nh,\n",
    "                       output_size=self.len_mdn_output,\n",
    "                       num_gaussians=self.num_gaussians,\n",
    "                       nz=self.nz)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self,a,z):\n",
    "        self.lstm.reset()\n",
    "        az = torch.cat((z,a),dim=2)\n",
    "        mus, sigmas, pis, hs = [],[],[],[]\n",
    "\n",
    "        for azi in az:\n",
    "            lstm_out,h = self.lstm(azi)\n",
    "            mu, sigma, pi = self.mdn(lstm_out[0])\n",
    "            \n",
    "            mus.append(mu[None,:])\n",
    "            sigmas.append(sigma[None,:])\n",
    "            pis.append(pi[None,:])\n",
    "            hs.append(h[None,:])\n",
    "        mus = torch.cat(mus)\n",
    "        sigmas = torch.cat(sigmas)\n",
    "        pis = torch.cat(pis)\n",
    "        hs = torch.cat(hs)\n",
    "\n",
    "        \n",
    "        return mus,sigmas,pis,hs\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def mdn_criterion(mus,sigmas,pis,label):\n",
    "    # z is batch of seq_len number of z's, where each z is nz long\n",
    "    #print(z.size())\n",
    "    # mus is for each element in the seqence, a batch of a mixture of num_guasians mean vectors, where each mean vector has nz elements\n",
    "    #print(mus.size())\n",
    "    # sigmas is for each element in the sequence,  a batch of a mixture of num_guassians covariance vectors, where each covariance vector has nz elements\n",
    "    # and represents the diagonal of the covariance matrix\n",
    "    #print(sigmas.size())\n",
    "    # we want to compute the density of a batch of z's under this batch of mixtures\n",
    "    # pad z with a dummy dimension to enable broadcasting over the num_mixture_components dimension\n",
    "    label = torch.unsqueeze(label,dim=2)\n",
    "    #print(z.size())\n",
    "    # we parametrize a normal distribution for every element in the sequence for every example in the batch for every mixture for every dimension\n",
    "    nd = torch.distributions.Normal(mus,sigmas)\n",
    "    # because the covariance matrix is diagonal the probability if z under a given mean vector and cov matrix is the product\n",
    "    # of the density of each element of z under a univariate guassian. For log prob, this turns into a sum. So\n",
    "    # if we sum in the dimension of the elements of z, then we get the log density of each sequence index for each example under each mixture\n",
    "    log_prob_elwise = nd.log_prob(label)\n",
    "    #print(log_prob_elwise.size())\n",
    "    log_prob = log_prob_elwise.sum(dim=-1)\n",
    "    #print(log_prob.size())\n",
    "    # pis is number of examples by mixture coefficients, so we can just elementwise multoply this with log_probs\n",
    "    # and sum along the mixture component direction\n",
    "    #print(pis.size())\n",
    "    NLL = -(pis * log_prob).sum(dim=-1)\n",
    "    # now we have negative log likelihood for each element in the sequence for each example in the batch\n",
    "    #print(NLL.size())\n",
    "\n",
    "    # now lets sum over each element in the sequence\n",
    "\n",
    "    seq_NLL = NLL.sum(dim=0)\n",
    "    #print(seq_NLL.size())\n",
    "    # now we take the mean over the batch\n",
    "    loss = seq_NLL.mean()\n",
    "    #print(loss.size())\n",
    "    return loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 1, 128, 256])\n",
      "864.1576538085938\n",
      "torch.Size([19, 1, 128, 256])\n",
      "864.3800048828125\n",
      "torch.Size([19, 1, 128, 256])\n",
      "865.6553955078125\n",
      "torch.Size([19, 1, 128, 256])\n",
      "868.129638671875\n",
      "torch.Size([19, 1, 128, 256])\n",
      "864.3246459960938\n",
      "torch.Size([19, 1, 128, 256])\n",
      "864.9536743164062\n",
      "torch.Size([19, 1, 128, 256])\n",
      "866.192138671875\n",
      "torch.Size([19, 1, 128, 256])\n",
      "866.29248046875\n",
      "torch.Size([19, 1, 128, 256])\n",
      "864.8701782226562\n",
      "torch.Size([19, 1, 128, 256])\n",
      "864.1880493164062\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "m = M().cuda()\n",
    "seq_len = 20\n",
    "opt = get_optim(\"adam\",  m, lr=0.001)\n",
    "for i in range(10):\n",
    "    m.zero_grad()\n",
    "    # we will have one more frame than action because we don't take an action after the last frame\n",
    "    z = Variable(torch.Tensor(seq_len,batch_size,m.nz).normal_()).cuda()\n",
    "    a = Variable(torch.Tensor(seq_len-1,batch_size,3).normal_()).cuda()\n",
    "    # here we push the az's through the rnn to get parameters of a mixture of guassians\n",
    "    # we don't throw the last z in there b/c it has no action for it\n",
    "    mus,sigmas,pis,hs = m(a,z[:-1])\n",
    "    print(hs.size())\n",
    "    #our label is the NEXT frame in the sequence, so the az that is input is matched with the next frame down\n",
    "    # so we don't need the first frame for our labels\n",
    "    label = z[1:]\n",
    "\n",
    "    loss = mdn_criterion(mus,sigmas,pis,label)\n",
    "    print(loss.data[0])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
